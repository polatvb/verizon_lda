{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hakanpolat/.local/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "/Users/hakanpolat/.local/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, defaultdict\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyodbc\n",
    "import re, collections\n",
    "import string\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pyLDAvis.gensim\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "import nltk.data\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from configparser import ConfigParser\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(myfile,sep='\\t',skiprows=(0,1,2),header=(0))\n",
    "datat = pd.read_csv('./Input/CA.txt',encoding=\" ISO-8859-1\", sep='\\t',header=(0))\n",
    "\n",
    "datac = pd.read_csv('./Input/CA.csv',encoding=\" ISO-8859-1\", sep='\\t',header=(0))\n",
    "\n",
    "datax = pd.read_excel('./Input/CA.xlsx')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index          80\n",
       "climsg      33792\n",
       "globalid    33792\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datat.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 ns ± 15.6 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit datat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                 80\n",
       "climsg,globalid    33792\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datac.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.24 ms ± 132 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit datac.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index          80\n",
       "climsg      33792\n",
       "globalid    33792\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datax.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.49 ms ± 317 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit datax.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pyAsterquery(text, maxlimit):\n",
    "    conn = pyodbc.connect('DRIVER={Aster ODBC Driver};SERVER =tdwtddpast27-2.tdc.vzwcorp.com;PORT = 2406;DATABASE=vzwdisc02;UID=mylavra;PWD=welcome02')\n",
    "    data = pd.read_sql(text + ' limit ' + str(maxlimit),conn)\n",
    "    return data\n",
    "\n",
    "def pySQL(schm, tbl, columns, cndtns):\n",
    "    return \"select \" + columns + \" from \" + schm + \".\" + tbl + \" where \" + cndtns\n",
    "\n",
    "def pyreadcsv(iname):\n",
    "    data = pd.read_csv('./Input/'+iname+'.csv')\n",
    "    return data\n",
    "\n",
    "def pyreadtxt(iname):\n",
    "    data = pd.read_csv('./Input/'+iname+'.txt',encoding=\" ISO-8859-1\", sep='\\t',header=(0) )\n",
    "\n",
    "def pyreadxl(iname):\n",
    "    data = pd.read_excel('./Input/'+iname+'.xlsx')\n",
    "    return data\n",
    "\n",
    "def pyremovenulls(df,msg):\n",
    "    df = df[df[msg].isnull()==False]\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    return df\n",
    "\n",
    "def applyfilters(colname, msgsearch, interacn):\n",
    "    interacn = ' ' + interacn + ' '\n",
    "    msgsearch = msgsearch.lower()\n",
    "    msgsearch = list(msgsearch.split(','))\n",
    "    for i in range(len(msgsearch)):\n",
    "        msgsearch[i] = colname + ' ilike ' + \"'\" +'%' + msgsearch[i] + '%' + \"'\"\n",
    "    return '(' + interacn.join(msgsearch) + ')'\n",
    "\n",
    "\n",
    "def pymsgsearchSQL(schm, tbl, columns, cndtns, colname, msgsearch,interacn):\n",
    "    return pySQL(schm, tbl, columns, cndtns) + ' and ' + applyfilters(colname, msgsearch,interacn)\n",
    "\n",
    "def pymsgSearchfile(df,msg,text,intrcn):\n",
    "    text = text.lower()\n",
    "    text = list(text.split(','))\n",
    "    if intrcn == 'and':\n",
    "        df['contains'] = df[msg].apply(lambda x: all(word in x.lower() for word in text))\n",
    "    else:\n",
    "        df['contains'] = df[msg].apply(lambda x: any(word in x.lower() for word in text))\n",
    "    df = df[df['contains'] == True]\n",
    "    df.drop(['contains'],axis = 1, inplace = True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    return df\n",
    "\n",
    "def pynametopics(df,msg,ntopics,npasses,topic_length):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    user_stopwords = pd.read_csv('./Input/Stopwords.txt',sep=',',encoding = \"ISO-8859-1\",header = 0)\n",
    "    def lemmatize_stemming(text):\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "    def preprocess(text):\n",
    "        result = [] \n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if (token not in STOPWORDS and token not in np.array(user_stopwords['Stopwords']) and len(token) > 3\n",
    "                and lemmatize_stemming(token) not in STOPWORDS and lemmatize_stemming(token) not in np.array(user_stopwords['Stopwords'])):\n",
    "                result.append(lemmatize_stemming(token))\n",
    "        return result\n",
    "    def run_lda(df,msg,ntopics,npasses):\n",
    "        chats_preprocessed = df[msg].apply(preprocess)\n",
    "        dictionary = gensim.corpora.Dictionary(chats_preprocessed)\n",
    "        dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in chats_preprocessed]\n",
    "        lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=ntopics, id2word=dictionary, passes=npasses)\n",
    "        topics = []\n",
    "        for i in range(len(bow_corpus)):\n",
    "            x = dict(lda_model.get_document_topics(bow_corpus[i]))\n",
    "            topics.append(max(x.keys(), key=(lambda key: x[key])))\n",
    "        df['topics'] = topics\n",
    "        return df, dictionary, bow_corpus, lda_model\n",
    "    df,dictionary,bow,lda = run_lda(df,msg,ntopics,npasses)\n",
    "    def get_topics(df,lda_model,ntopics,topic_length):\n",
    "        topic_words = []\n",
    "        for i in range(ntopics):\n",
    "            x = lda_model.print_topic(i,topn=7)\n",
    "            topic_words.append(re.sub('[^a-zA-Z]+',' ',x).split(' ')[1:-1])\n",
    "        d = {}\n",
    "        for j in range(7):\n",
    "            for i in range(ntopics):\n",
    "                if topic_words[i][j] not in d:\n",
    "                    d[topic_words[i][j]] = i\n",
    "        a = np.array(list(d.values()))\n",
    "        b = np.array(list(d.keys()))\n",
    "        topics_name = []\n",
    "        for i in range(ntopics):\n",
    "            topics_name.append(' & '.join(list(b[np.where(a == i)])[:topic_length]))\n",
    "        df['topics'] = df['topics'].apply(lambda x: topics_name[x])\n",
    "        df['topics'] = df['topics'].apply(lambda x: x.upper())\n",
    "        return df, topic_words, topics_name\n",
    "    df, topic_words, topic_names = get_topics(df,lda,ntopics,topic_length)\n",
    "    # Save model to disk.\n",
    "    # temp_file = datapath(\"model\")\n",
    "    # lda.save(temp_file)\n",
    "\n",
    "    # Load a potentially pretrained model from disk.\n",
    "    # lda = LdaModel.load(temp_file)\n",
    "    return df,dictionary,bow,lda,topic_words,topic_names\n",
    "def pySentiment(df,msg):\n",
    "    def pyScore(score):\n",
    "        if score > 0.1:\n",
    "            return 'Positive'\n",
    "        elif score < -0.1:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    df['Score'] = df[msg].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['Sentiment'] = df['Score'].apply(lambda x:pyScore(x))\n",
    "    df.drop(['Score'],axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "def pygetcurrenttime():\n",
    "    x = str(datetime.datetime.now()).replace(' ','_')\n",
    "    x = x.replace(':','')\n",
    "    x = x.replace('.','')\n",
    "    return x\n",
    "\n",
    "def pysavedata(df,typ,x):\n",
    "    if typ == 'csv':\n",
    "        df.to_csv('./Output/Data/'+x+'.csv',sep = ',',index=False)\n",
    "    elif typ == 'xlsx':\n",
    "        df.to_excel('./Output/Data/'+x+'.xlsx',index=False)\n",
    "\n",
    "def pyLDAvissave(lda,bow,dictionary,x):\n",
    "    p = pyLDAvis.gensim.prepare(lda, bow, dictionary,sort_topics = True)\n",
    "    pyLDAvis.save_html(p,'./Output/pyLDAVis/'+x+'.html')\n",
    "\n",
    "def pytopicsbarvis(df,x):\n",
    "    sns.set(rc={'figure.figsize':(25,8.27)})\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    sns.set_context(\"paper\", font_scale=1.5)\n",
    "    total = float(len(df)) # one person per row\n",
    "    ax = sns.countplot(x=\"topics\", data=df,order = df['topics'].value_counts().index)\n",
    "    ax.set_title('Topic Frequency',fontsize=20)\n",
    "    ax.set_xlabel('Top Topics',fontsize=16)\n",
    "    ax.set_ylabel('Percentage(%)',fontsize=16)\n",
    "    ax.legend(loc='upper left',fontsize=20)\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.3f}'.format(int(height/total*100)),\n",
    "            ha=\"center\")\n",
    "    # if you do not want to display it, and just to save it, use below code\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    ax.get_figure().savefig('./Output/Plots/frequency/Frequency_'+x+'.png')\n",
    "\n",
    "def pysentbarvis(df,x):\n",
    "    sns.set(rc={'figure.figsize':(28,8.27)})\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    sns.set_context(\"paper\", font_scale=1.5)\n",
    "    total = float(len(df)) # one person per row\n",
    "    ax = sns.countplot(x=\"topics\", hue=\"Sentiment\", data=df,order = df['topics'].value_counts().index)\n",
    "    ax.set_title('Sentiment Analysis',fontsize=20)\n",
    "    ax.set_xlabel('TOPICS',fontsize=20)\n",
    "    ax.set_ylabel('Percentage(%)',fontsize=20)\n",
    "    ax.legend(loc='upper left',fontsize=20,labels=['Neutral','Positive','Negative'])\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.3f}'.format(height/total*100),\n",
    "            ha=\"center\")\n",
    "    # if you do not want to display it, and just to save it, use below code\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    ax.get_figure().savefig('./Output/Plots/sentiment/Sentiment_'+x+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'dprelated2' does not exist: b'dprelated2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-38d9c00313ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0minput_typ\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'txt'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'file_name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mfilter_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'filter_data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilter_data\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'dprelated2' does not exist: b'dprelated2'"
     ]
    }
   ],
   "source": [
    "parser = ConfigParser()\n",
    "parser.read('LDA_Config.ini')\n",
    "\n",
    "input_typ = parser.get('Input_Type','input_typ')\n",
    "colname = parser.get('colname','colname')\n",
    "if input_typ == 'Custom_SQL':\n",
    "    df = pyAsterquery(parser.get('Custom_SQL','custom_sql'),parser.get('Max_Rows_SQL','max_limit'))\n",
    "elif input_typ == 'Predefined_SQL':\n",
    "    schema_name = parser.get('Predefined_SQL','schema_name')\n",
    "    table_name = parser.get('Predefined_SQL','table_name')\n",
    "    columns = parser.get('Predefined_SQL','columns')\n",
    "    conditions = parser.get('Predefined_SQL','conditions')\n",
    "    filter_data = parser.get('Predefined_SQL','filter_data')\n",
    "    max_rows = parser.get('Max_Rows_SQL','max_limit')\n",
    "    if filter_data == True:\n",
    "        filter_values = parser.get('Predefined_SQL','filter_values')\n",
    "        interaction = parser.get('Predefined_SQL','interaction')\n",
    "        sql_query = pymsgsearchSQL(schema_name, table_name, columns, conditions, colname, filter_values,interaction)\n",
    "        df = pyAsterquery(sql_query,max_rows)\n",
    "    else:\n",
    "        sql_query = pySQL(schema_name, table_name, columns, conditions)\n",
    "        df = pyAsterquery(sql_query,max_rows)\n",
    "elif input_typ == 'csv':\n",
    "    filename = parser.get('csv','file_name')\n",
    "    df = pyreadcsv(filename)\n",
    "    filter_data = parser.get('csv','filter_data')\n",
    "    if filter_data == True:\n",
    "        filter_values = parser.get('csv','filter_values')\n",
    "        interaction = parser.get('csv','interaction')\n",
    "        df = pymsgSearchfile(df,colname,filter_values,interaction)\n",
    "elif input_typ == 'txt':\n",
    "    filename = parser.get('txt','file_name')\n",
    "    df = pd.read_table(filename)\n",
    "    filter_data = filter_data = parser.get('txt','filter_data')\n",
    "    if filter_data == True:\n",
    "        filter_values = parser.get('txt','filter_values')\n",
    "        interaction = parser.get('txt','interaction')\n",
    "        df = pymsgSearchfile(df,colname,filter_values,interaction)\n",
    "    \n",
    "elif input_typ == 'xlsx':\n",
    "    filename = parser.get('xlsx','file_name')\n",
    "    df = pyreadxl(filename)\n",
    "    filter_data = parser.get('xlsx','filter_data')\n",
    "    if filter_data == True:\n",
    "        filter_values = parser.get('xlsx','filter_values')\n",
    "        interaction = parser.get('xlsx','interaction')\n",
    "        df = pymsgSearchfile(df,colname,filter_values,interaction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    df['climsg'] = df['climsg'].apply(str)\n",
    "    df = pyremovenulls(df,colname)\n",
    "#     df = df.dropna(subset=['msg'])\n",
    "    num_topics = parser.get('LDA_Topic_Modeling','num_topics')\n",
    "    num_passes = parser.get('LDA_Topic_Modeling','num_passes')\n",
    "    topic_length = parser.get('Topic_Length','topic_length')\n",
    "    df,dictionary,bow,lda,topic_words,topic_names = pynametopics(df,colname,int(num_topics),int(num_passes),int(topic_length))\n",
    "    df = pySentiment(df,colname)\n",
    "    current_date_time = pygetcurrenttime()\n",
    "    pysavedata(df,parser.get('save_data','type'),current_date_time)\n",
    "    pyLDAvissave(lda,bow,dictionary,current_date_time)\n",
    "    pytopicsbarvis(df,current_date_time)\n",
    "    pysentbarvis(df,current_date_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['credit', 'watch', 'order', 'check', 'payment', 'transfer', 'applic'],\n",
       " ['charg', 'pay', 'dollar', 'switch', 'plan', 'cost', 'wonder'],\n",
       " ['plan', 'discount', 'unlimit', 'data', 'switch', 'price', 'offer'],\n",
       " ['store', 'order', 'pick', 'ship', 'case', 'protect', 'promo'],\n",
       " ['trade', 'cart', 'order', 'account', 'credit', 'balanc', 'pay'],\n",
       " ['payment', 'price', 'watch', 'plan', 'contract', 'pay', 'deal'],\n",
       " ['order', 'account', 'address', 'ship', 'space', 'payment', 'confirm']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['credit & watch',\n",
       " 'charg & pay',\n",
       " 'plan & discount',\n",
       " 'store & pick',\n",
       " 'trade & cart',\n",
       " 'payment & price',\n",
       " 'order & account']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.113*\"credit\" + 0.075*\"watch\" + 0.045*\"order\" + 0.034*\"check\" + 0.029*\"payment\" + 0.014*\"transfer\" + 0.014*\"applic\" + 0.013*\"ask\" + 0.013*\"trade\" + 0.012*\"save\"'),\n",
       " (1,\n",
       "  '0.066*\"charg\" + 0.056*\"pay\" + 0.044*\"dollar\" + 0.042*\"switch\" + 0.042*\"plan\" + 0.027*\"cost\" + 0.027*\"wonder\" + 0.026*\"tax\" + 0.025*\"account\" + 0.025*\"fee\"'),\n",
       " (2,\n",
       "  '0.170*\"plan\" + 0.084*\"discount\" + 0.060*\"unlimit\" + 0.032*\"data\" + 0.030*\"switch\" + 0.018*\"price\" + 0.017*\"offer\" + 0.016*\"atampampt\" + 0.014*\"share\" + 0.014*\"deal\"'),\n",
       " (3,\n",
       "  '0.124*\"store\" + 0.108*\"order\" + 0.059*\"pick\" + 0.028*\"ship\" + 0.027*\"case\" + 0.022*\"protect\" + 0.017*\"promo\" + 0.015*\"date\" + 0.014*\"deliv\" + 0.014*\"deliveri\"'),\n",
       " (4,\n",
       "  '0.176*\"trade\" + 0.063*\"cart\" + 0.042*\"order\" + 0.034*\"account\" + 0.024*\"credit\" + 0.022*\"balanc\" + 0.019*\"pay\" + 0.017*\"checkout\" + 0.014*\"return\" + 0.014*\"tradein\"'),\n",
       " (5,\n",
       "  '0.096*\"payment\" + 0.047*\"price\" + 0.037*\"watch\" + 0.034*\"plan\" + 0.028*\"contract\" + 0.028*\"pay\" + 0.026*\"deal\" + 0.025*\"add\" + 0.023*\"care\" + 0.019*\"devic\"'),\n",
       " (6,\n",
       "  '0.085*\"order\" + 0.080*\"account\" + 0.053*\"address\" + 0.042*\"ship\" + 0.024*\"space\" + 0.023*\"payment\" + 0.021*\"confirm\" + 0.019*\"cart\" + 0.018*\"enter\" + 0.017*\"check\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if (token not in STOPWORDS and token not in np.array(user_stopwords['Stopwords']) and len(token) > 3 \n",
    "            and lemmatize_stemming(token) not in STOPWORDS and lemmatize_stemming(token) not in np.array(user_stopwords['Stopwords'])):\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "user_stopwords = pd.read_csv('./Input/Stopwords.txt',sep=',',encoding = \"ISO-8859-1\",header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpc = pd.read_excel('cpc.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3016"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cpc['climsg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('plan', 'unlimit', 'data'), 6),\n",
       " (('talk', 'real', 'person'), 6),\n",
       " (('unlimit', 'data', 'plan'), 5),\n",
       " (('offer', 'unlimit', 'plan'), 4),\n",
       " (('best', 'deal', 'deal'), 2),\n",
       " (('inform', 'intern', 'plan'), 2),\n",
       " (('inform', 'price', 'price'), 2),\n",
       " (('deal', 'requir', 'deal'), 2),\n",
       " (('deal', 'incent', 'deal'), 2),\n",
       " (('data', 'roll', 'data'), 2),\n",
       " (('expir', 'expir', 'date'), 2),\n",
       " (('reward', 'expir', 'expir'), 2),\n",
       " (('day', 'data', 'leav'), 2),\n",
       " (('unlimit', 'talk', 'data'), 2),\n",
       " (('data', 'plan', 'unlimit'), 2),\n",
       " (('claim', 'insur', 'claim'), 2),\n",
       " (('respons', 'interfac', 'excel'), 2),\n",
       " (('interfac', 'excel', 'account'), 2),\n",
       " (('excel', 'account', 'inform'), 2),\n",
       " (('account', 'inform', 'clear'), 2),\n",
       " (('inform', 'clear', 'excel'), 2),\n",
       " (('clear', 'excel', 'construct'), 2),\n",
       " (('excel', 'construct', 'improv'), 2),\n",
       " (('construct', 'improv', 'interact'), 2),\n",
       " (('cost', 'plan', 'pay'), 2),\n",
       " (('loyal', 'deal', 'incent'), 2),\n",
       " (('offer', 'requir', 'smartphon'), 2),\n",
       " (('guess', 'switch', 'deal'), 2),\n",
       " (('easi', 'easi', 'navig'), 2),\n",
       " (('suck', 'loyalti', 'mean'), 2),\n",
       " (('offer', 'best', 'deal'), 2),\n",
       " (('deal', 'requir', 'account'), 2),\n",
       " (('offer', 'filter', 'featur'), 2),\n",
       " (('older', 'unlimit', 'plan'), 2),\n",
       " (('motorola', 'smartphon', 'motorola'), 2),\n",
       " (('account', 'request', 'account'), 2),\n",
       " (('unlimit', 'plan', 'internet'), 2),\n",
       " (('quick', 'easi', 'disappoint'), 2),\n",
       " (('store', 'hat', 'gut'), 2),\n",
       " (('offer', 'deal', 'loyal'), 2)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''\n",
    "for i in cpc['climsg']:\n",
    "    text = text + str(i)\n",
    "    \n",
    "text = preprocess(text)\n",
    "#get a list of all the n-grams\n",
    "esBigrams = ngrams(text, 3)\n",
    "# get the frequency of each bigram in our corpus\n",
    "esBigramFreq = collections.Counter(esBigrams)\n",
    "# what are the ten most popular ngrams?\n",
    "esBigramFreq.most_common(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
